alg_name: "ROME"
model_name: "microsoft/biogpt-large"
stats_dir: "./data/stats"
device: 0
layers: [17]
fact_token: "subject_last"
v_num_grad_steps: 20
v_lr: 5e-1
v_loss_layer: 47
v_weight_decay: 0.5
clamp_norm_factor: 4
kl_factor: 0.0625
mom2_adjustment: false
context_template_length_params: [[5, 10], [10, 10]]
rewrite_module_tmp: "biogpt.layers.{}.fc2"
layer_module_tmp: "biogpt.layers.{}"
mlp_module_tmp: "biogpt.layers.{}.fc2"
attn_module_tmp: "biogpt.layers.{}.self_attn"
ln_f_module: biogpt.layer_norm
lm_head_module: "embed_tokens"
mom2_dataset: "wikipedia"
mom2_n_samples: 100000
mom2_dtype: "float32"
model_parallel: false
max_length: 200



# BioGptForCausalLM(
#   (biogpt): BioGptModel(
#     (embed_tokens): Embedding(57717, 1600, padding_idx=1)
#     (embed_positions): BioGptLearnedPositionalEmbedding(2050, 1600)
#     (layers): ModuleList(
#       (0-47): 48 x BioGptDecoderLayer(
#         (self_attn): BioGptAttention(
#           (k_proj): Linear(in_features=1600, out_features=1600, bias=True)
#           (v_proj): Linear(in_features=1600, out_features=1600, bias=True)
#           (q_proj): Linear(in_features=1600, out_features=1600, bias=True)
#           (out_proj): Linear(in_features=1600, out_features=1600, bias=True)
#         )
#         (activation_fn): GELUActivation()
#         (self_attn_layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
#         (fc1): Linear(in_features=1600, out_features=6400, bias=True)
#         (fc2): Linear(in_features=6400, out_features=1600, bias=True)
#         (final_layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
#       )
#     )
#     (layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
#   )
#   (output_projection): Linear(in_features=1600, out_features=57717, bias=False)
# )


